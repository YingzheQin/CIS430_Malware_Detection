"""
Trains a PyTorch image classification model using device-agnostic code.
"""
import argparse
from timeit import default_timer as timer
from torch.utils.tensorboard.writer import SummaryWriter
from datetime import datetime
from pathlib import Path
import torch
from torch import nn
from torchvision.transforms import v2

import data_setup, engine, model_builder, utils

# Create a parser
parser = argparse.ArgumentParser(description='Get some hyperparameters')

# Get an arg for num_epochs
parser.add_argument('--num_epochs',
                    default=10,
                    type=int,
                    help='the number of epochs to train')

# Get an arg for batch_size
parser.add_argument('--batch_size',
                    default=32,
                    type=int,
                    help='the number of epochs to train for')

# Get an arg for hidden units
parser.add_argument('--hidden_units',
                    default=10,
                    type=int,
                    help='number of hidden units in hidden layers')

# Get an arg for learning rate
parser.add_argument('--learning_rate',
                    default=0.001,
                    type=float,
                    help='learning rate to use for model')
# Get an arg for training directory
parser.add_argument('--train_dir',
                    default='4_percent_image_data/train',
                    type=str,
                    help='directory file path to training data in standard image classification format')
# Get an arg for testing directory
parser.add_argument('--test_dir',
                    default='4_percent_image_data/test',
                    type=str,
                    help='directory file path to testing data in standard image classification format')
# Get our arguments from the parser
args = parser.parse_args()

# Setup hyperparameters
NUM_EPOCHS = args.num_epochs
BATCH_SIZE = args.batch_size
HIDDEN_UNITS = args.hidden_units
LEARNING_RATE = args.learning_rate
print(
    f"[INFO] Training a model for {NUM_EPOCHS} epochs with batch size {BATCH_SIZE} using {HIDDEN_UNITS} hidden units and a learning rate of {LEARNING_RATE}")
# Setup directories
train_dir = args.train_dir
test_dir = args.test_dir
print(f"[INFO] Training data file: {train_dir}")
print(f"[INFO] Testing data file: {test_dir}")

# Setup target device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Create transforms
data_transform = v2.Compose([
    v2.Resize((32, 32)),  # first 1024 bytes  1024=32*32
    v2.ToImage(),
    v2.ToDtype(torch.float32, scale=True)
])
# Create Dataloaders with help from data_setup.py
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(
    train_dir=train_dir,
    test_dir=test_dir,
    transform=data_transform,
    batch_size=BATCH_SIZE
)


def create_writer(experiment_name: str,
                  model_name: str,
                  extra: str = None,
                  ) -> SummaryWriter():
    """
    Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.
    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.
    Where timestamp is the current date in YYYY-MM-DD format.

    Args:
        experiment_name: Name of experiment
        model_name: Name of model
        extra:  Anything extra to add to the directory. Defaults to None.

    Returns: torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.

    Example usage:
    # Create a writer saving to "runs/2023-11-20/data_10_percent/effnetb2/5_epochs/"
    writer = create_writer(experiment_name="data_10_percent",
                           model_name="effnetb2",
                           extra="5_epochs")
    # The above is the same as:
    writer = SummaryWriter(log_dir="runs/2023-11-20/data_10_percent/effnetb2/5_epochs/")
    """
    timestamp = datetime.now().strftime('%Y-%m-%d')
    log_dir = Path('runs') / timestamp / experiment_name / model_name
    if extra:
        log_dir /= extra
    print(f"[INFO] Created SummaryWriter, saving to: {log_dir}...")
    return SummaryWriter(log_dir=str(log_dir))


# Create model with help from model_builder.py
torch.manual_seed(42)
torch.cuda.manual_seed(42)
model_0 = model_builder.TinyVGG(input_shape=3,
                                hidden_units=HIDDEN_UNITS,
                                output_shape=len(class_names)).to(device)

# Set loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model_0.parameters(),
                             lr=LEARNING_RATE)
start_time = timer()

# Start training with help of engine.py
engine.train(model=model_0,
             train_dataloader=train_dataloader,
             test_dataloader=test_dataloader,
             optimizer=optimizer,
             loss_fn=loss_fn,
             epochs=NUM_EPOCHS,
             device=device,
             writer=create_writer(experiment_name='my_test',
                                  model_name='TinyVGG',
                                  ))
end_time = timer()
print(f'[INFO] Total training time: {end_time - start_time:.3f} seconds')

# Save the model with help from utils.py
utils.save_model(model=model_0,
                 target_dir='models',
                 model_name='TinyVGG_model.pth')
